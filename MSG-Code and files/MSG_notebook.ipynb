{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GramMatrix(nn.Module):\n",
    "    def forward(self, y):\n",
    "        (_, ch, h, w) = y.size()\n",
    "        features = y.view(_, ch, w * h)\n",
    "        gram = torch.bmm(features, features.transpose(1, 2)) / (ch * h * w)\n",
    "        return gram\n",
    "\n",
    "class CoMatch(nn.Module):\n",
    "    def __init__(self, C):\n",
    "        super(CoMatch, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(1, C, C), requires_grad=True)\n",
    "        self.Gram = Variable(torch.Tensor(1, C, C), requires_grad=True)\n",
    "        self.C = C\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.weight.data.uniform_(0.0, 0.02)\n",
    "\n",
    "    def setTarget(self, target):\n",
    "        self.Gram = target\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.Q = torch.bmm(self.weight.expand_as(self.Gram), self.Gram)\n",
    "        matching =  torch.bmm(self.Q.transpose(1, 2).expand(x.size(0), self.C, self.C), x.view(x.size(0), x.size(1), -1)).view_as(x)\n",
    "        return matching \n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + 'N x ' + str(self.C) + ')'\n",
    "\n",
    "class ConvLayer(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        reflection_padding = (math.floor(kernel_size/2))\n",
    "        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.reflection_pad(x)\n",
    "        out = self.conv2d(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UpsampleConvLayer(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
    "        super(UpsampleConvLayer, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        if upsample:\n",
    "            self.upsample_layer = torch.nn.Upsample(scale_factor=upsample)\n",
    "        self.reflection_padding = (math.floor(kernel_size/2))\n",
    "        if self.reflection_padding != 0:\n",
    "            self.reflection_pad = nn.ReflectionPad2d(self.reflection_padding)\n",
    "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.upsample:\n",
    "            x = self.upsample_layer(x)\n",
    "        if self.reflection_padding != 0:\n",
    "            x = self.reflection_pad(x)\n",
    "        out = self.conv2d(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.expansion = 4\n",
    "        self.downsample = downsample\n",
    "        if self.downsample is not None:\n",
    "            self.residual_layer = nn.Conv2d(inplanes, planes*self.expansion,\n",
    "                                            kernel_size=1, stride=stride)\n",
    "        self.conv_block = nn.Sequential(nn.BatchNorm2d(inplanes),\n",
    "                       nn.ReLU(inplace=True),\n",
    "                       nn.Conv2d(inplanes, planes, kernel_size=1, stride=1),\n",
    "                       nn.BatchNorm2d(planes),\n",
    "                       nn.ReLU(inplace=True),\n",
    "                       ConvLayer(planes, planes, kernel_size=3, stride=stride),\n",
    "                       nn.BatchNorm2d(planes),\n",
    "                       nn.ReLU(inplace=True),\n",
    "                       nn.Conv2d(planes, planes*self.expansion, kernel_size=1,stride=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.downsample is None:\n",
    "            residual = x\n",
    "        else:\n",
    "            residual = self.residual_layer(x)       \n",
    "        out = self.conv_block(x)\n",
    "        final = out + residual\n",
    "        return final \n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inplanes, planes, stride=2):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.expansion = 4\n",
    "        self.residual_layer = UpsampleConvLayer(inplanes, planes*self.expansion,\n",
    "                                                kernel_size=1, stride=1,\n",
    "                                                upsample=stride)\n",
    "        \n",
    "        self.conv_block = nn.Sequential(nn.BatchNorm2d(inplanes),\n",
    "                       nn.ReLU(inplace=True),\n",
    "                       nn.Conv2d(inplanes, planes, kernel_size=1, stride=1),\n",
    "                       nn.BatchNorm2d(planes),\n",
    "                       nn.ReLU(inplace=True),\n",
    "                       UpsampleConvLayer(planes, planes, kernel_size=3, stride=1, upsample=stride),\n",
    "                       nn.BatchNorm2d(planes),\n",
    "                       nn.ReLU(inplace=True),\n",
    "                       nn.Conv2d(planes, planes*self.expansion, kernel_size=1,stride=1))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x) + self.residual_layer(x)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_nc=3, output_nc=3, ngf=64,\n",
    "                 n_blocks=6):\n",
    "        super(Net, self).__init__()\n",
    "        self.Gram = GramMatrix()\n",
    "        block = Bottleneck\n",
    "        upblock = ResidualBlock\n",
    "        expansion = 4\n",
    "\n",
    "        self.siamese = nn.Sequential(ConvLayer(input_nc, 64, kernel_size=7, stride=1),\n",
    "                   nn.BatchNorm2d(64),\n",
    "                   nn.ReLU(inplace=True),\n",
    "                   block(64, 32, 2, 1),\n",
    "                   block(32*expansion, ngf, 2, 1))\n",
    "\n",
    "        self.ins = CoMatch(ngf*expansion)\n",
    "        model = [self.siamese]\n",
    "        model += [self.ins]\n",
    "\n",
    "        for i in range(n_blocks):\n",
    "            model += [block(ngf*expansion, ngf, 1, None)]\n",
    "\n",
    "        model += [upblock(ngf*expansion, 32, 2),\n",
    "                  upblock(32*expansion, 16, 2),\n",
    "                  nn.BatchNorm2d(16*expansion),\n",
    "                  nn.ReLU(inplace=True),\n",
    "                  ConvLayer(16*expansion, output_nc, kernel_size=7, stride=1)]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def setTarget(self, Xs):\n",
    "        f = self.siamese(Xs)\n",
    "        G = self.Gram(f)\n",
    "        self.ins.setTarget(G)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "def tensor_load_rgbimage(filename, size=None):\n",
    "    img = Image.open(filename).convert('RGB')\n",
    "    img = img.resize((size, size), Image.ANTIALIAS)\n",
    "    img = np.array(img).transpose(2, 0, 1)\n",
    "    img = torch.from_numpy(img).float()\n",
    "    return img\n",
    "\n",
    "\n",
    "def tensor_save_rgbimage(tensor, filename, cuda=False):\n",
    "    if cuda:\n",
    "        img = tensor.clone().cpu().clamp(0, 255).numpy()\n",
    "    else:\n",
    "        img = tensor.clone().clamp(0, 255).numpy()\n",
    "    img = img.transpose(1, 2, 0).astype('uint8')\n",
    "    img = Image.fromarray(img)\n",
    "    img.save(filename)\n",
    "\n",
    "\n",
    "def tensor_save_bgrimage(tensor, filename, cuda=False):\n",
    "    (b, g, r) = torch.chunk(tensor, 3)\n",
    "    tensor = torch.cat((r, g, b))\n",
    "    tensor_save_rgbimage(tensor, filename, cuda)\n",
    "\n",
    "\n",
    "def preprocess_batch(batch):\n",
    "    batch = batch.transpose(0, 1)\n",
    "    (r, g, b) = torch.chunk(batch, 3)\n",
    "    batch = torch.cat((b, g, r))\n",
    "    batch = batch.transpose(0, 1)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code in this block has been directly taken from https://github.com/zhanghang1989/PyTorch-Multi-Style-Transfer\n",
    "\n",
    "#this can be uncommented after you have downloaded this once\n",
    "!wget -q -O 21styles.model https://www.dropbox.com/s/2iz8orqqubrfrpo/21styles.model?dl=1\n",
    "\n",
    "style_model = Net(ngf=128)\n",
    "model_dict = torch.load('21styles.model')\n",
    "model_dict_clone = model_dict.copy()\n",
    "for key, value in model_dict_clone.items():\n",
    "    if key.endswith(('running_mean', 'running_var')):\n",
    "        del model_dict[key]\n",
    "style_model.load_state_dict(model_dict, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@article{zhang2017multistyle,\n",
    "\ttitle={Multi-style Generative Network for Real-time Transfer},\n",
    "\tauthor={Zhang, Hang and Dana, Kristin},\n",
    "\tjournal={arXiv preprint arXiv:1703.06953},\n",
    "\tyear={2017}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code below for generating single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = tensor_load_rgbimage('st.jpg', size=512).unsqueeze(0)\n",
    "style_image = tensor_load_rgbimage('mosaicc.jpg', size=512).unsqueeze(0)\n",
    "style_image = preprocess_batch(style_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namesaved = 'final3.jpg'\n",
    "style_v = Variable(style_image)\n",
    "content_image = Variable(preprocess_batch(content_image))\n",
    "style_model.setTarget(style_v)\n",
    "#print(content_image)\n",
    "output = style_model(content_image)\n",
    "tensor_save_bgrimage(output.data[0], namesaved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image.open(namesaved))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code below for combining style and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_pics = [\"Composition\", \"nude\", \"Scream\", \"Shipwreck_turner\",\"simple\",\"st\",\"sunrise\",\"yellow\"]\n",
    "content_pics = [\"ballet\", \"chapel\", \"df\", \"face\", \"house\", \"landscape\", \"Taj\", \"tsunami\"]\n",
    "for i in range(len(style_pics)):\n",
    "    for j in range(len(content_pics)):\n",
    "        if (content_pics[j] == \"Taj\"):\n",
    "            content_image = tensor_load_rgbimage('content/'+str(content_pics[j])+'.jpeg', size=512).unsqueeze(0)\n",
    "        else:\n",
    "            content_image = tensor_load_rgbimage('content/'+str(content_pics[j])+'.jpg', size=512).unsqueeze(0)\n",
    "        if (style_pics[i] == \"nude\"):\n",
    "            style_image = tensor_load_rgbimage('style/'+str(style_pics[i])+'.jpeg', size=512).unsqueeze(0)\n",
    "        else:\n",
    "            style_image = tensor_load_rgbimage('style/'+str(style_pics[i])+'.jpg', size=512).unsqueeze(0)\n",
    "        style_image = preprocess_batch(style_image)\n",
    "        \n",
    "        style_model = Net(ngf=128)\n",
    "        model_dict = torch.load('21styles.model')\n",
    "        model_dict_clone = model_dict.copy()\n",
    "        for key, value in model_dict_clone.items():\n",
    "            if key.endswith(('running_mean', 'running_var')):\n",
    "                del model_dict[key]\n",
    "        style_model.load_state_dict(model_dict, False)\n",
    "    \n",
    "        style_v = Variable(style_image)\n",
    "        content_image = Variable(preprocess_batch(content_image))\n",
    "        style_model.setTarget(style_v)\n",
    "        output = style_model(content_image)\n",
    "        tensor_save_bgrimage(output.data[0], 'output/'+str(content_pics[j])+'_'+str(style_pics[i])+'.jpg', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code below for dif brush sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes_list =  [64, 128, 256, 512, 1024]\n",
    "for i in range(len(sizes_list)):\n",
    "    content_image = tensor_load_rgbimage('chapel.jpg', size=512).unsqueeze(0)\n",
    "    style_image = tensor_load_rgbimage('Composition.jpg', size = sizes_list[i]).unsqueeze(0)\n",
    "    style_image = preprocess_batch(style_image)\n",
    "    style_model = Net(ngf=128)\n",
    "    model_dict = torch.load('21styles.model')\n",
    "    model_dict_clone = model_dict.copy()\n",
    "    for key, value in model_dict_clone.items():\n",
    "        if key.endswith(('running_mean', 'running_var')):\n",
    "            del model_dict[key]\n",
    "    style_model.load_state_dict(model_dict, False)\n",
    "    namesaved = 'brushsize_' + str(i) + '.jpg'\n",
    "    style_v = Variable(style_image)\n",
    "    content_image = Variable(preprocess_batch(content_image))\n",
    "    style_model.setTarget(style_v)\n",
    "    output = style_model(content_image)\n",
    "    tensor_save_bgrimage(output.data[0], namesaved)\n",
    "    display(Image.open(namesaved))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
